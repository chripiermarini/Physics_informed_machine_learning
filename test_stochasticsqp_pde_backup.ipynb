{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "We have been given a PDE: $\\frac{du(x,t)}{dx}=2\\frac{du(x,t)}{dt}+u(x,t)$\n",
    "and boundary condition: $u(x,0)=6e^{-3x}$\n",
    "\n",
    "- Independent variables (input): $(x,t)$ \n",
    "- pde solution (outputs): $u(x,t)$ \n",
    "- Let $\\bar{u}(x,t)$ as the neural network-predicted pde solution at $(x,t)$\n",
    "\n",
    "\n",
    "We want to use nueral network to accurately predict pde solution for all $x$ in range $[0,2]$ and $t$ in range $[0,1]$\n",
    "\n",
    "\n",
    "When we solved this pde analytically, we found the solution: $u(x,t) = 6e^{-3x-2t}$\n",
    "\n",
    "### Generate Sample\n",
    "- Define $S_{B}:=\\{(x_i,0)\\}_{i=1}^{n_b}$ as a set of sample where, for $i \\in [1,n_b]$, $(x_i,0)$ is a sample point on the boundary.\n",
    "- Define $S_{I}:=\\{(x_i,t_i)\\}_{i=1}^{m}$ as a set of sample where, for $i \\in [1,m]$, $(x_i,t_i)$ is a sample point in the interior of $[0,2]\\times[0,1]$.\n",
    "\n",
    "\n",
    "### Constrained Machine Learning\n",
    "The objective is to minimize mse loss of predicted pde solution and true pde solution of boundary sample points, i.e.,  \n",
    "$$\n",
    "\\frac{1}{n_b}\\sum_{(x_i,0) \\in S_B} \\|\\bar{u}(x_i,0) - u(x_i,0)\\|^2.\n",
    "$$\n",
    "The constraints are pde is satisfied for all interior sample points, i.e.,\n",
    "$$\n",
    "\\frac{d\\bar{u}(x_i,t_i)}{dx}-2\\frac{d\\bar{u}(x_i,t_i)}{dt}-\\bar{u}(x_i,t_i) = 0 \\text{ for all } (x_i, t_i) \\in S_I\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T17:06:35.966997400Z",
     "start_time": "2024-02-02T17:06:35.954281Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from stochasticsqp import *\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "import numpy as np\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T17:06:36.596566900Z",
     "start_time": "2024-02-02T17:06:36.584164600Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "When forming the network, we have to keep in mind the number of inputs and outputs\n",
    "In our case: #inputs = 2 (x,t)\n",
    "and #outputs = 1\n",
    "\n",
    "You can add as many hidden layers as you want with as many neurons.\n",
    "More complex the network, the more prepared it is to find complex solutions, but it also requires more data.\n",
    "\n",
    "Let us create this network: 2 hidden layer with 16 neurons each.\n",
    "\"\"\"\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.n_input = 2\n",
    "        self.n_neurons = 16\n",
    "        self.n_output = 1\n",
    "        self.hidden_layer1 = nn.Linear(self.n_input,self.n_neurons)\n",
    "        self.hidden_layer2 = nn.Linear(self.n_neurons,self.n_neurons)\n",
    "        self.output_layer = nn.Linear(self.n_neurons,self.n_output)\n",
    "\n",
    "    def forward(self, x,t):\n",
    "        inputs = torch.cat([x,t],axis=1) # combined two arrays of 1 columns each to one array of 2 columns\n",
    "        layer1_out = torch.sigmoid(self.hidden_layer1(inputs))\n",
    "        layer2_out = torch.sigmoid(self.hidden_layer2(layer1_out))\n",
    "        output = self.output_layer(layer2_out) ## For regression, no activation is used in output layer\n",
    "        return output\n",
    "    \n",
    "    \n",
    "## PDE as constraint function. Thus would use the network\n",
    "## For general constraint, you can implenet whatever function of input below \n",
    "def constraint_func(x,t, net):\n",
    "    u = net(x,t) # the dependent variable u is given by the network based on independent variables x,t\n",
    "    ## Based on PDE du/dx - 2du/dt - u = 0, we need to compute du/dx and du/dt\n",
    "    u_x = torch.autograd.grad(u.sum(), x, create_graph=True)[0]\n",
    "    u_t = torch.autograd.grad(u.sum(), t, create_graph=True)[0]\n",
    "    pde = u_x - 2*u_t - u\n",
    "    return pde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T17:06:37.102967500Z",
     "start_time": "2024-02-02T17:06:37.090677700Z"
    }
   },
   "outputs": [],
   "source": [
    "## Generate Sample\n",
    "\n",
    "# Boundary sample for objective\n",
    "n_obj_sample = 500\n",
    "x_bc = np.random.uniform(low=0.0, high=2.0, size=(n_obj_sample,1))\n",
    "t_bc = np.zeros((n_obj_sample,1))\n",
    "u_bc = 6*np.exp(-3*x_bc)\n",
    "\n",
    "# Interior sample for constraints (no need pde true solution)\n",
    "n_constrs = 10\n",
    "x_collocation = np.random.uniform(low=0.0, high=2.0, size=(n_constrs,1))\n",
    "t_collocation = np.random.uniform(low=0.0, high=1.0, size=(n_constrs,1))\n",
    "\n",
    "##  Model\n",
    "net = Net()\n",
    "net = net.to(device)\n",
    "mse_cost_function = torch.nn.MSELoss() # Mean squared error\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "n_parameters = count_parameters(net)\n",
    "\n",
    "## Initialize optimizer\n",
    "optimizer = StochasticSQP(net.parameters(),\n",
    "                          lr=0.001,\n",
    "                          n_parameters = n_parameters, \n",
    "                          n_constrs = n_constrs,\n",
    "                          merit_param_init = 1, \n",
    "                          ratio_param_init = 1)\n",
    "\n",
    "## Construct tensor \n",
    "pt_x_bc = Variable(torch.from_numpy(x_bc).float(), requires_grad=False).to(device)\n",
    "pt_t_bc = Variable(torch.from_numpy(t_bc).float(), requires_grad=False).to(device)\n",
    "pt_u_bc = Variable(torch.from_numpy(u_bc).float(), requires_grad=False).to(device)\n",
    "pt_x_collocation = Variable(torch.from_numpy(x_collocation).float(), requires_grad=True).to(device)\n",
    "pt_t_collocation = Variable(torch.from_numpy(t_collocation).float(), requires_grad=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T17:06:37.566072400Z",
     "start_time": "2024-02-02T17:06:37.507230200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iter        Loss       ||c||     merit_f    stepsize merit_param ratio_param \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "transpose() received an invalid combination of arguments - got (Tensor), but expected one of:\n * (Tensor input, int dim0, int dim1)\n * (Tensor input, name dim0, name dim1)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 41\u001B[0m\n\u001B[0;32m     38\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstate[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mdata\n\u001B[0;32m     40\u001B[0m \u001B[38;5;66;03m# Take a step inside optimizer\u001B[39;00m\n\u001B[1;32m---> 41\u001B[0m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;66;03m# Print out\u001B[39;00m\n\u001B[0;32m     44\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mprinterIteration(every\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\env1\\Lib\\site-packages\\torch\\optim\\optimizer.py:385\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    380\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    381\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    382\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    383\u001B[0m             )\n\u001B[1;32m--> 385\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    386\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[0;32m    388\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\Dottorato\\Qi_Curtis_project\\Stochastic_SQP\\stochasticsqp.py:75\u001B[0m, in \u001B[0;36mStochasticSQP.step\u001B[1;34m(self, closure)\u001B[0m\n\u001B[0;32m     70\u001B[0m d \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39msolve(ls_matrix, ls_rhs)\n\u001B[0;32m     72\u001B[0m \u001B[38;5;66;03m## Update merit parameter\u001B[39;00m\n\u001B[0;32m     73\u001B[0m \n\u001B[0;32m     74\u001B[0m \u001B[38;5;66;03m## define trial merit parameter\u001B[39;00m\n\u001B[1;32m---> 75\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mmm(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtranspose\u001B[49m\u001B[43m(\u001B[49m\u001B[43mg\u001B[49m\u001B[43m)\u001B[49m, d) \u001B[38;5;241m+\u001B[39m torch\u001B[38;5;241m.\u001B[39mmax(np\u001B[38;5;241m.\u001B[39mmm(np\u001B[38;5;241m.\u001B[39mmm(torch\u001B[38;5;241m.\u001B[39mtranspose(d),H),d), \u001B[38;5;241m0\u001B[39m) \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m     76\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrial_merit\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m10\u001B[39m\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[1;31mTypeError\u001B[0m: transpose() received an invalid combination of arguments - got (Tensor), but expected one of:\n * (Tensor input, int dim0, int dim1)\n * (Tensor input, name dim0, name dim1)\n"
     ]
    }
   ],
   "source": [
    "## Main optimization loop\n",
    "\n",
    "iterations = 10000\n",
    "optimizer.printerHeader()\n",
    "for epoch in range(iterations):\n",
    "    \n",
    "    # Compute loss (objective)\n",
    "    net_bc_out = net(pt_x_bc, pt_t_bc) \n",
    "    loss = mse_cost_function(net_bc_out, pt_u_bc)\n",
    "    \n",
    "    # Compute gradient of objective\n",
    "    g = torch.zeros(n_parameters)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    i=0\n",
    "    for name, param in net.named_parameters():\n",
    "        grad_l = len(param.grad.view(-1))\n",
    "        g[i:i+grad_l] = param.grad.view(-1)\n",
    "        i += grad_l\n",
    "    \n",
    "    # Compute constraints\n",
    "    c = constraint_func(pt_x_collocation, pt_t_collocation, net).reshape(-1) \n",
    "\n",
    "    # Compute Jacobian\n",
    "    J = torch.zeros(n_constrs, n_parameters)\n",
    "    for i in range(n_constrs):\n",
    "        optimizer.zero_grad()\n",
    "        c[i].backward(retain_graph=True)\n",
    "        grads = torch.Tensor() #dict()\n",
    "        for name, param in net.named_parameters():\n",
    "            grads = torch.cat((grads, param.grad.view(-1)),0)\n",
    "        J[i,:] = grads\n",
    "\n",
    "    # Update f, g, c, J to optimizer\n",
    "    optimizer.state['J'] = J\n",
    "    optimizer.state['c'] = c.data\n",
    "    optimizer.state['g'] = g\n",
    "    optimizer.state['f'] = loss.data\n",
    "\n",
    "    # Take a step inside optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print out\n",
    "    optimizer.printerIteration(every=100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
