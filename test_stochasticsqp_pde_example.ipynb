{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "We have been given a PDE: $\\frac{du(x,t)}{dx}=2\\frac{du(x,t)}{dt}+u(x,t)$\n",
    "and boundary condition: $u(x,0)=6e^{-3x}$\n",
    "\n",
    "- Independent variables (input): $(x,t)$ \n",
    "- pde solution (outputs): $u(x,t)$ \n",
    "- Let $\\bar{u}(x,t)$ as the neural network-predicted pde solution at $(x,t)$\n",
    "\n",
    "\n",
    "We want to use nueral network to accurately predict pde solution for all $x$ in range $[0,2]$ and $t$ in range $[0,1]$\n",
    "\n",
    "\n",
    "When we solved this pde analytically, we found the solution: $u(x,t) = 6e^{-3x-2t}$\n",
    "\n",
    "### Generate Sample\n",
    "- Define $S_{B}:=\\{(x_i,0)\\}_{i=1}^{n_b}$ as a set of sample where, for $i \\in [1,n_b]$, $(x_i,0)$ is a sample point on the boundary.\n",
    "- Define $S_{I}:=\\{(x_i,t_i)\\}_{i=1}^{m}$ as a set of sample where, for $i \\in [1,m]$, $(x_i,t_i)$ is a sample point in the interior of $[0,2]\\times[0,1]$.\n",
    "\n",
    "\n",
    "### Constrained Machine Learning\n",
    "The objective is to minimize mse loss of predicted pde solution and true pde solution of boundary sample points, i.e.,  \n",
    "$$\n",
    "\\frac{1}{n_b}\\sum_{(x_i,0) \\in S_B} \\|\\bar{u}(x_i,0) - u(x_i,0)\\|^2.\n",
    "$$\n",
    "The constraints are pde is satisfied for all interior sample points, i.e.,\n",
    "$$\n",
    "\\frac{d\\bar{u}(x_i,t_i)}{dx}-2\\frac{d\\bar{u}(x_i,t_i)}{dt}-\\bar{u}(x_i,t_i) = 0 \\text{ for all } (x_i, t_i) \\in S_I\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from stochasticsqp import *\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "import numpy as np\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "When forming the network, we have to keep in mind the number of inputs and outputs\n",
    "In our case: #inputs = 2 (x,t)\n",
    "and #outputs = 1\n",
    "\n",
    "You can add as many hidden layers as you want with as many neurons.\n",
    "More complex the network, the more prepared it is to find complex solutions, but it also requires more data.\n",
    "\n",
    "Let us create this network: 2 hidden layer with 16 neurons each.\n",
    "\"\"\"\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.n_input = 2\n",
    "        self.n_neurons = 16\n",
    "        self.n_output = 1\n",
    "        self.hidden_layer1 = nn.Linear(self.n_input,self.n_neurons)\n",
    "        self.hidden_layer2 = nn.Linear(self.n_neurons,self.n_neurons)\n",
    "        self.output_layer = nn.Linear(self.n_neurons,self.n_output)\n",
    "\n",
    "    def forward(self, x,t):\n",
    "        inputs = torch.cat([x,t],axis=1) # combined two arrays of 1 columns each to one array of 2 columns\n",
    "        layer1_out = torch.sigmoid(self.hidden_layer1(inputs))\n",
    "        layer2_out = torch.sigmoid(self.hidden_layer2(layer1_out))\n",
    "        output = self.output_layer(layer2_out) ## For regression, no activation is used in output layer\n",
    "        return output\n",
    "    \n",
    "    \n",
    "## PDE as constraint function. Thus would use the network\n",
    "## For general constraint, you can implenet whatever function of input below \n",
    "def constraint_func(x,t, net):\n",
    "    u = net(x,t) # the dependent variable u is given by the network based on independent variables x,t\n",
    "    ## Based on PDE du/dx - 2du/dt - u = 0, we need to compute du/dx and du/dt\n",
    "    u_x = torch.autograd.grad(u.sum(), x, create_graph=True)[0]\n",
    "    u_t = torch.autograd.grad(u.sum(), t, create_graph=True)[0]\n",
    "    pde = u_x - 2*u_t - u\n",
    "    return pde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate Sample\n",
    "\n",
    "# Boundary sample for objective\n",
    "n_obj_sample = 500\n",
    "x_bc = np.random.uniform(low=0.0, high=2.0, size=(n_obj_sample,1))\n",
    "t_bc = np.zeros((n_obj_sample,1))\n",
    "u_bc = 6*np.exp(-3*x_bc)\n",
    "\n",
    "# Interior sample for constraints (no need pde true solution)\n",
    "n_constrs = 10\n",
    "x_collocation = np.random.uniform(low=0.0, high=2.0, size=(n_constrs,1))\n",
    "t_collocation = np.random.uniform(low=0.0, high=1.0, size=(n_constrs,1))\n",
    "\n",
    "##  Model\n",
    "net = Net()\n",
    "net = net.to(device)\n",
    "mse_cost_function = torch.nn.MSELoss() # Mean squared error\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "n_parameters = count_parameters(net)\n",
    "\n",
    "## Initialize optimizer\n",
    "optimizer = StochasticSQP(net.parameters(),\n",
    "                          lr=0.001,\n",
    "                          n_parameters = n_parameters, \n",
    "                          n_constrs = n_constrs,\n",
    "                          merit_param_init = 1, \n",
    "                          ratio_param_init = 1)\n",
    "\n",
    "## Construct tensor \n",
    "pt_x_bc = Variable(torch.from_numpy(x_bc).float(), requires_grad=False).to(device)\n",
    "pt_t_bc = Variable(torch.from_numpy(t_bc).float(), requires_grad=False).to(device)\n",
    "pt_u_bc = Variable(torch.from_numpy(u_bc).float(), requires_grad=False).to(device)\n",
    "pt_x_collocation = Variable(torch.from_numpy(x_collocation).float(), requires_grad=True).to(device)\n",
    "pt_t_collocation = Variable(torch.from_numpy(t_collocation).float(), requires_grad=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iter        Loss       ||c||     merit_f    stepsize merit_param ratio_param\n",
      "       0  2.4315e+00  3.4839e+00  5.9154e+00  1.0000e-03  1.0000e+00  1.0000e+00\n",
      "     100  2.4757e+00  3.1522e+00  5.6279e+00  1.0000e-03  1.0000e+00  1.0000e+00\n",
      "     200  2.5176e+00  2.8521e+00  5.3696e+00  1.0000e-03  1.0000e+00  1.0000e+00\n",
      "     300  2.5610e+00  2.5782e+00  5.1392e+00  1.0000e-03  1.0000e+00  1.0000e+00\n",
      "     400  2.5974e+00  2.3328e+00  4.9302e+00  1.0000e-03  1.0000e+00  1.0000e+00\n",
      "     500  2.6319e+00  2.1106e+00  4.7426e+00  1.0000e-03  1.0000e+00  1.0000e+00\n",
      "     600  2.6619e+00  1.9095e+00  4.5714e+00  1.0000e-03  1.0000e+00  1.0000e+00\n",
      "     700  2.6910e+00  1.7278e+00  4.4187e+00  1.0000e-03  1.0000e+00  1.0000e+00\n",
      "     800  2.7179e+00  1.5633e+00  4.2813e+00  1.0000e-03  1.0000e+00  1.0000e+00\n",
      "     900  2.7433e+00  1.4145e+00  4.1578e+00  1.0000e-03  1.0000e+00  1.0000e+00\n",
      "    1000  2.7670e+00  1.2795e+00  4.0465e+00  1.0000e-03  1.0000e+00  1.0000e+00\n",
      "    1100  2.7863e+00  1.1576e+00  3.9438e+00  1.0000e-03  1.0000e+00  1.0000e+00\n",
      "    1200  2.8049e+00  1.0473e+00  3.8522e+00  1.0000e-03  1.0000e+00  1.0000e+00\n",
      "    1300  2.8085e+00  1.2442e+00  4.0527e+00  5.0000e-04  1.0000e+00  1.0000e+00\n",
      "    1400  2.8138e+00  1.2024e+00  4.0162e+00  2.5000e-04  1.0000e+00  1.0000e+00\n",
      "    1500  2.8174e+00  1.1727e+00  3.9901e+00  2.5000e-04  1.0000e+00  1.0000e+00\n",
      "    1600  2.8214e+00  1.1437e+00  3.9651e+00  2.5000e-04  1.0000e+00  1.0000e+00\n",
      "    1700  2.8257e+00  1.1155e+00  3.9412e+00  2.5000e-04  1.0000e+00  1.0000e+00\n",
      "    1800  2.8296e+00  1.0880e+00  3.9175e+00  2.5000e-04  1.0000e+00  1.0000e+00\n",
      "    1900  2.8327e+00  1.0609e+00  3.8936e+00  2.5000e-04  1.0000e+00  1.0000e+00\n",
      "    2000  2.8351e+00  1.0469e+00  3.8820e+00  1.2500e-04  1.0000e+00  1.0000e+00\n",
      "    2100  2.8370e+00  1.0339e+00  3.8709e+00  1.2500e-04  1.0000e+00  1.0000e+00\n",
      "    2200  2.8383e+00  1.0210e+00  3.8593e+00  1.2500e-04  1.0000e+00  1.0000e+00\n",
      "    2300  2.8401e+00  1.0083e+00  3.8484e+00  1.2500e-04  1.0000e+00  1.0000e+00\n",
      "    2400  2.8415e+00  9.9903e-01  3.8406e+00  6.2500e-05  1.0000e+00  1.0000e+00\n",
      "    2500  2.8425e+00  9.9281e-01  3.8353e+00  6.2500e-05  1.0000e+00  1.0000e+00\n",
      "    2600  2.8433e+00  9.8661e-01  3.8299e+00  6.2500e-05  1.0000e+00  1.0000e+00\n",
      "    2700  2.8442e+00  9.8148e-01  3.8257e+00  3.1250e-05  1.0000e+00  1.0000e+00\n",
      "    2800  2.8447e+00  9.7841e-01  3.8231e+00  3.1250e-05  1.0000e+00  1.0000e+00\n",
      "    2900  2.8451e+00  9.7535e-01  3.8205e+00  3.1250e-05  1.0000e+00  1.0000e+00\n",
      "    3000  2.8456e+00  9.7231e-01  3.8179e+00  3.1250e-05  1.0000e+00  1.0000e+00\n",
      "    3100  2.8460e+00  9.6927e-01  3.8153e+00  3.1250e-05  1.0000e+00  1.0000e+00\n",
      "    3200  2.8464e+00  9.6624e-01  3.8127e+00  3.1250e-05  1.0000e+00  1.0000e+00\n",
      "    3300  2.8468e+00  9.6323e-01  3.8101e+00  3.1250e-05  1.0000e+00  1.0000e+00\n",
      "    3400  2.8473e+00  9.6022e-01  3.8075e+00  3.1250e-05  1.0000e+00  1.0000e+00\n",
      "    3500  2.8477e+00  9.5723e-01  3.8049e+00  3.1250e-05  1.0000e+00  1.0000e+00\n",
      "    3600  2.8481e+00  9.5423e-01  3.8024e+00  3.1250e-05  1.0000e+00  1.0000e+00\n",
      "    3700  2.8484e+00  9.5225e-01  3.8007e+00  1.5625e-05  1.0000e+00  1.0000e+00\n",
      "    3800  2.8487e+00  9.5077e-01  3.7994e+00  1.5625e-05  1.0000e+00  1.0000e+00\n",
      "    3900  2.8489e+00  9.4928e-01  3.7982e+00  1.5625e-05  1.0000e+00  1.0000e+00\n",
      "    4000  2.8490e+00  9.4823e-01  3.7973e+00  7.8125e-06  1.0000e+00  1.0000e+00\n",
      "    4100  2.8491e+00  9.4749e-01  3.7966e+00  7.8125e-06  1.0000e+00  1.0000e+00\n",
      "    4200  2.8492e+00  9.4675e-01  3.7960e+00  7.8125e-06  1.0000e+00  1.0000e+00\n",
      "    4300  2.8493e+00  9.4620e-01  3.7955e+00  3.9063e-06  1.0000e+00  1.0000e+00\n",
      "    4400  2.8494e+00  9.4583e-01  3.7952e+00  3.9063e-06  1.0000e+00  1.0000e+00\n",
      "    4500  2.8494e+00  9.4546e-01  3.7949e+00  3.9063e-06  1.0000e+00  1.0000e+00\n",
      "    4600  2.8495e+00  9.4509e-01  3.7946e+00  3.9063e-06  1.0000e+00  1.0000e+00\n",
      "    4700  2.8495e+00  9.4472e-01  3.7943e+00  3.9063e-06  1.0000e+00  1.0000e+00\n",
      "    4800  2.8496e+00  9.4435e-01  3.7939e+00  3.9063e-06  1.0000e+00  1.0000e+00\n",
      "    4900  2.8496e+00  9.4399e-01  3.7936e+00  3.9063e-06  1.0000e+00  1.0000e+00\n",
      "    5000  2.8497e+00  9.4362e-01  3.7933e+00  3.9063e-06  1.0000e+00  1.0000e+00\n",
      "    5100  2.8498e+00  9.4326e-01  3.7930e+00  1.9531e-06  1.0000e+00  1.0000e+00\n",
      "    5200  2.8498e+00  9.4307e-01  3.7929e+00  1.9531e-06  1.0000e+00  1.0000e+00\n",
      "    5300  2.8498e+00  9.4289e-01  3.7927e+00  1.9531e-06  1.0000e+00  1.0000e+00\n",
      "    5400  2.8498e+00  9.4271e-01  3.7925e+00  1.9531e-06  1.0000e+00  1.0000e+00\n",
      "    5500  2.8499e+00  9.4252e-01  3.7924e+00  1.9531e-06  1.0000e+00  1.0000e+00\n",
      "    5600  2.8499e+00  9.4234e-01  3.7922e+00  1.9531e-06  1.0000e+00  1.0000e+00\n",
      "    5700  2.8499e+00  9.4216e-01  3.7921e+00  1.9531e-06  1.0000e+00  1.0000e+00\n",
      "    5800  2.8499e+00  9.4197e-01  3.7919e+00  1.9531e-06  1.0000e+00  1.0000e+00\n",
      "    5900  2.8500e+00  9.4179e-01  3.7918e+00  1.9531e-06  1.0000e+00  1.0000e+00\n",
      "    6000  2.8500e+00  9.4161e-01  3.7916e+00  1.9531e-06  1.0000e+00  1.0000e+00\n",
      "    6100  2.8500e+00  9.4142e-01  3.7914e+00  1.9531e-06  1.0000e+00  1.0000e+00\n",
      "    6200  2.8500e+00  9.4124e-01  3.7913e+00  1.9531e-06  1.0000e+00  1.0000e+00\n",
      "    6300  2.8501e+00  9.4106e-01  3.7911e+00  1.9531e-06  1.0000e+00  1.0000e+00\n",
      "    6400  2.8501e+00  9.4087e-01  3.7910e+00  1.9531e-06  1.0000e+00  1.0000e+00\n",
      "    6500  2.8501e+00  9.4069e-01  3.7908e+00  1.9531e-06  1.0000e+00  1.0000e+00\n",
      "    6600  2.8501e+00  9.4050e-01  3.7907e+00  1.9531e-06  1.0000e+00  1.0000e+00\n",
      "    6700  2.8502e+00  9.4032e-01  3.7905e+00  1.9531e-06  1.0000e+00  1.0000e+00\n",
      "    6800  2.8502e+00  9.4014e-01  3.7903e+00  1.9531e-06  1.0000e+00  1.0000e+00\n",
      "    6900  2.8502e+00  9.3996e-01  3.7902e+00  1.9531e-06  1.0000e+00  1.0000e+00\n",
      "    7000  2.8503e+00  9.3977e-01  3.7900e+00  1.9531e-06  1.0000e+00  1.0000e+00\n",
      "    7100  2.8503e+00  9.3959e-01  3.7899e+00  1.9531e-06  1.0000e+00  1.0000e+00\n",
      "    7200  2.8503e+00  9.3941e-01  3.7897e+00  1.9531e-06  1.0000e+00  1.0000e+00\n",
      "    7300  2.8503e+00  9.3922e-01  3.7896e+00  1.9531e-06  1.0000e+00  1.0000e+00\n",
      "    7400  2.8504e+00  9.3904e-01  3.7894e+00  1.9531e-06  1.0000e+00  1.0000e+00\n",
      "    7500  2.8504e+00  9.3886e-01  3.7892e+00  1.9531e-06  1.0000e+00  1.0000e+00\n",
      "    7600  2.8504e+00  9.3868e-01  3.7891e+00  1.9531e-06  1.0000e+00  1.0000e+00\n",
      "    7700  2.8504e+00  9.3849e-01  3.7889e+00  1.9531e-06  1.0000e+00  1.0000e+00\n",
      "    7800  2.8505e+00  9.3831e-01  3.7888e+00  1.9531e-06  1.0000e+00  1.0000e+00\n",
      "    7900  2.8505e+00  9.3812e-01  3.7886e+00  1.9531e-06  1.0000e+00  1.0000e+00\n",
      "    8000  2.8505e+00  9.3794e-01  3.7885e+00  1.9531e-06  1.0000e+00  1.0000e+00\n",
      "    8100  2.8505e+00  9.3776e-01  3.7883e+00  1.9531e-06  1.0000e+00  1.0000e+00\n",
      "    8200  2.8506e+00  9.3758e-01  3.7881e+00  1.9531e-06  1.0000e+00  1.0000e+00\n",
      "    8300  2.8506e+00  9.3739e-01  3.7880e+00  1.9531e-06  1.0000e+00  1.0000e+00\n",
      "    8400  2.8506e+00  9.3721e-01  3.7878e+00  1.9531e-06  1.0000e+00  1.0000e+00\n",
      "    8500  2.8506e+00  9.3703e-01  3.7877e+00  1.9531e-06  1.0000e+00  1.0000e+00\n",
      "    8600  2.8507e+00  9.3684e-01  3.7875e+00  1.9531e-06  1.0000e+00  1.0000e+00\n",
      "    8700  2.8507e+00  9.3666e-01  3.7874e+00  1.9531e-06  1.0000e+00  1.0000e+00\n",
      "    8800  2.8507e+00  9.3655e-01  3.7873e+00  9.7656e-07  1.0000e+00  1.0000e+00\n",
      "    8900  2.8507e+00  9.3646e-01  3.7872e+00  9.7656e-07  1.0000e+00  1.0000e+00\n",
      "    9000  2.8507e+00  9.3637e-01  3.7871e+00  9.7656e-07  1.0000e+00  1.0000e+00\n",
      "    9100  2.8508e+00  9.3628e-01  3.7870e+00  9.7656e-07  1.0000e+00  1.0000e+00\n",
      "    9200  2.8508e+00  9.3619e-01  3.7870e+00  9.7656e-07  1.0000e+00  1.0000e+00\n",
      "    9300  2.8508e+00  9.3610e-01  3.7869e+00  9.7656e-07  1.0000e+00  1.0000e+00\n",
      "    9400  2.8508e+00  9.3601e-01  3.7868e+00  9.7656e-07  1.0000e+00  1.0000e+00\n",
      "    9500  2.8508e+00  9.3594e-01  3.7867e+00  4.8828e-07  1.0000e+00  1.0000e+00\n",
      "    9600  2.8508e+00  9.3590e-01  3.7867e+00  1.2207e-07  1.0000e+00  1.0000e+00\n",
      "    9700  2.8508e+00  9.3590e-01  3.7867e+00  6.1035e-08  1.0000e+00  1.0000e+00\n",
      "    9800  2.8508e+00  9.3590e-01  3.7867e+00  6.1035e-08  1.0000e+00  1.0000e+00\n",
      "    9900  2.8508e+00  9.3590e-01  3.7867e+00  3.0518e-08  1.0000e+00  1.0000e+00\n"
     ]
    }
   ],
   "source": [
    "## Main optimization loop\n",
    "\n",
    "iterations = 10000\n",
    "optimizer.printerHeader()\n",
    "for epoch in range(iterations):\n",
    "    \n",
    "    # Compute loss (objective)\n",
    "    net_bc_out = net(pt_x_bc, pt_t_bc) \n",
    "    loss = mse_cost_function(net_bc_out, pt_u_bc)\n",
    "    \n",
    "    # Compute gradient of objective\n",
    "    g = torch.zeros(n_parameters)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    i=0\n",
    "    for name, param in net.named_parameters():\n",
    "        grad_l = len(param.grad.view(-1))\n",
    "        g[i:i+grad_l] = param.grad.view(-1)\n",
    "        i += grad_l\n",
    "    \n",
    "    # Compute constraints\n",
    "    c = constraint_func(pt_x_collocation, pt_t_collocation, net).reshape(-1) \n",
    "\n",
    "    # Compute Jacobian\n",
    "    J = torch.zeros(n_constrs, n_parameters)\n",
    "    for i in range(n_constrs):\n",
    "        optimizer.zero_grad()\n",
    "        c[i].backward(retain_graph=True)\n",
    "        grads = torch.Tensor() #dict()\n",
    "        for name, param in net.named_parameters():\n",
    "            grads = torch.cat((grads, param.grad.view(-1)),0)\n",
    "        J[i,:] = grads\n",
    "\n",
    "    # Update f, g, c, J to optimizer\n",
    "    optimizer.state['J'] = J\n",
    "    optimizer.state['c'] = c.data\n",
    "    optimizer.state['g'] = g\n",
    "    optimizer.state['f'] = loss.data\n",
    "\n",
    "    # Take a step inside optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print out\n",
    "    optimizer.printerIteration(every=100)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
