{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement Spring\n",
    "\n",
    "We have been given a PDE: $\\frac{du(x,t)}{dx}=2\\frac{du(x,t)}{dt}+u(x,t)$\n",
    "and boundary condition: $u(x,0)=6e^{-3x}$\n",
    "\n",
    "- Independent variables (input): $(x,t)$ \n",
    "- pde solution (outputs): $u(x,t)$ \n",
    "- Let $\\bar{u}(x,t)$ as the neural network-predicted pde solution at $(x,t)$\n",
    "\n",
    "\n",
    "We want to use a neural network to accurately predict pde solution for all $x$ in range $[0,2]$ and $t$ in range $[0,1]$\n",
    "\n",
    "\n",
    "When we solved this pde analytically, we found the solution: $u(x,t) = 6e^{-3x-2t}$\n",
    "\n",
    "### Generate Sample\n",
    "- Define $S_{B}:=\\{(x_i,0)\\}_{i=1}^{n_b}$ as a set of sample where, for $i \\in [1,n_b]$, $(x_i,0)$ is a sample point on the boundary.\n",
    "- Define $S_{I}:=\\{(x_i,t_i)\\}_{i=1}^{m}$ as a set of sample where, for $i \\in [1,m]$, $(x_i,t_i)$ is a sample point in the interior of $[0,2]\\times[0,1]$.\n",
    "\n",
    "\n",
    "### Constrained Machine Learning\n",
    "The objective is to minimize mse loss of predicted pde solution and true pde solution of boundary sample points, i.e.,  \n",
    "$$\n",
    "\\frac{1}{n_b}\\sum_{(x_i,0) \\in S_B} \\|\\bar{u}(x_i,0) - u(x_i,0)\\|^2.\n",
    "$$\n",
    "The constraints are pde is satisfied for all interior sample points, i.e.,\n",
    "$$\n",
    "\\frac{d\\bar{u}(x_i,t_i)}{dx}-2\\frac{d\\bar{u}(x_i,t_i)}{dt}-\\bar{u}(x_i,t_i) = 0 \\text{ for all } (x_i, t_i) \\in S_I\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Problem Statement Darcy\n",
    "\n",
    "Given a function $u(x)$, we define the residual on the 2D Darcy Flow PDE as the following:\n",
    "\\begin{equation}\n",
    "    \\mathcal{F}(u(x)) = -\\nabla \\cdot(\\nu(x)\\nabla u(x)) - f(x) = 0,\n",
    "\\end{equation}\n",
    "where $\\nu \\in L^{\\infty}((0,1)^{2}; \\mathbb{R})$ is a diffusion coefficient and $f \\in L^{2}((0,1)^{2}; \\mathbb{R})$ is the forcing function. We are using $v(x)=2$ and $f(x) = 1$ in our first settings.\n",
    "The boundary conditions are simply $u(x,0)= 0 \\quad \\forall {x_{i}} \\in \\partial(0,1)^{2}$.\n",
    "- Independent variables (input): $(x1,x2) = x$ \n",
    "- pde solution (outputs): $u(x1,x2) = u(x)$ \n",
    "- Let $\\bar{u}(x)$ as the neural network-predicted pde solution at $(x)$\n",
    "- \n",
    "### Generate Sample\n",
    "- Define $S_{x}:=\\{(x1_i,x2_i)\\}_{i=1}^{n_b}$ as a set of sample where, for $i \\in [1,n_b]$, $(x1_i,x2_i)$ is a sample point in the box of $[0,1]\\times[0,1]$.\n",
    "- We have to be able to separate the interior from the boundary.\n",
    "\n",
    "### Constrained Machine Learning\n",
    "Given $N$ data points $x_{i}$ where $i=1, \\dots, N$, the optimization problem through which the neural net can approximate a solution of 2D Darcy Flow PDE should be the following:\n",
    "\\begin{equation}\n",
    "    \\begin{aligned}\n",
    "\n",
    "    &\\min_{x_{i} \\in (0,1)^{2}} & \\frac{1}{N} \\sum_{i=1}^{N} | \\mathcal{F}(u(x_{i})) |\\\\\n",
    "    &s.t. &u(x_{i}) = 0, \\quad \\forall {x_{i}} \\in \\partial(0,1)^{2}.       \n",
    "    \\end{aligned}\n",
    "\\end{equation}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-12T13:53:05.704460300Z",
     "start_time": "2024-02-12T13:53:05.664378700Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from stochasticsqp import *\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "import numpy as np\n",
    "torch.manual_seed(10000)\n",
    "np.random.seed(10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-12T13:53:05.736180700Z",
     "start_time": "2024-02-12T13:53:05.681051700Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "When forming the network, we have to keep in mind the number of inputs and outputs\n",
    "In our case: #inputs = 2 (x,t)\n",
    "and #outputs = 1\n",
    "\n",
    "You can add as many hidden layers as you want with as many neurons.\n",
    "More complex the network, the more prepared it is to find complex solutions, but it also requires more data.\n",
    "\n",
    "Let us create this network: 2 hidden layer with 16 neurons each.\n",
    "\"\"\"\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.n_input = 2\n",
    "        self.n_neurons = 16\n",
    "        self.n_output = 1\n",
    "        self.hidden_layer1 = nn.Linear(self.n_input,self.n_neurons)\n",
    "        self.hidden_layer2 = nn.Linear(self.n_neurons,self.n_neurons)\n",
    "        self.output_layer = nn.Linear(self.n_neurons,self.n_output)\n",
    "\n",
    "    def forward(self, x,t):\n",
    "        inputs = torch.cat([x,t],axis=1) # combined two arrays of 1 columns each to one array of 2 columns\n",
    "        layer1_out = torch.sigmoid(self.hidden_layer1(inputs))\n",
    "        layer2_out = torch.sigmoid(self.hidden_layer2(layer1_out))\n",
    "        output = self.output_layer(layer2_out) ## For regression, no activation is used in output layer\n",
    "        return output\n",
    "    \n",
    "    \n",
    "## PDE as constraint function. Thus would use the network\n",
    "## For general constraint, you can implenet whatever function of input below \n",
    "def constraint_func(x,t, net):\n",
    "    u = net(x,t) # the dependent variable u is given by the network based on independent variables x,t\n",
    "    ## Based on PDE du/dx - 2du/dt - u = 0, we need to compute du/dx and du/dt\n",
    "    u_x = torch.autograd.grad(u.sum(), x, create_graph=True)[0]\n",
    "    u_t = torch.autograd.grad(u.sum(), t, create_graph=True)[0]\n",
    "    pde = u_x - 2*u_t - u\n",
    "    return pde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-12T13:53:05.738633800Z",
     "start_time": "2024-02-12T13:53:05.717419900Z"
    }
   },
   "outputs": [],
   "source": [
    "## Generate Sample\n",
    "\n",
    "# Boundary sample for objective\n",
    "n_obj_sample = 500\n",
    "x_bc = np.random.uniform(low=0.0, high=2.0, size=(n_obj_sample,1))\n",
    "t_bc = np.zeros((n_obj_sample,1))\n",
    "u_bc = 6*np.exp(-3*x_bc)\n",
    "\n",
    "# Interior sample for constraints (no need pde true solution)\n",
    "n_constrs = 10\n",
    "x_collocation = np.random.uniform(low=0.0, high=2.0, size=(n_constrs,1))\n",
    "t_collocation = np.random.uniform(low=0.0, high=1.0, size=(n_constrs,1))\n",
    "\n",
    "##  Model\n",
    "net = Net()\n",
    "net = net.to(device)\n",
    "mse_cost_function = torch.nn.MSELoss() # Mean squared error\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "n_parameters = count_parameters(net)\n",
    "\n",
    "## Initialize optimizer\n",
    "optimizer = StochasticSQP(net.parameters(),\n",
    "                          lr=0.001,\n",
    "                          n_parameters = n_parameters, \n",
    "                          n_constrs = n_constrs,\n",
    "                          merit_param_init = 1, \n",
    "                          ratio_param_init = 1)\n",
    "\n",
    "## Construct tensor \n",
    "pt_x_bc = Variable(torch.from_numpy(x_bc).float(), requires_grad=False).to(device)\n",
    "pt_t_bc = Variable(torch.from_numpy(t_bc).float(), requires_grad=False).to(device)\n",
    "pt_u_bc = Variable(torch.from_numpy(u_bc).float(), requires_grad=False).to(device)\n",
    "pt_x_collocation = Variable(torch.from_numpy(x_collocation).float(), requires_grad=True).to(device)\n",
    "pt_t_collocation = Variable(torch.from_numpy(t_collocation).float(), requires_grad=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-12T13:54:05.647239700Z",
     "start_time": "2024-02-12T13:53:05.747843400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iter        Loss       ||c||     merit_f    stepsize merit_param ratio_param trial_merit trial_ratio\n",
      "       0  2.9186e+00  2.2487e+00  2.2487e+00  1.0000e-03  6.4261e-13  1.0000e+00  1.2852e-12  3.5000e+00\n",
      "     100  1.9398e+04  3.4067e+03  3.4067e+03  9.7656e-07  6.4261e-13  1.0000e+00  1.3054e-03  2.9752e+09\n",
      "     200  1.9415e+04  3.4066e+03  3.4066e+03  2.9802e-11  6.4261e-13  1.0000e+00  6.8868e-04  1.8674e+09\n",
      "     300  1.9415e+04  3.4066e+03  3.4066e+03  2.9802e-11  6.4261e-13  1.0000e+00  6.8868e-04  1.8674e+09\n",
      "     400  1.9415e+04  3.4066e+03  3.4066e+03  2.9802e-11  6.4261e-13  1.0000e+00  6.8868e-04  1.8674e+09\n",
      "     500  1.9415e+04  3.4066e+03  3.4066e+03  2.9802e-11  6.4261e-13  1.0000e+00  6.8868e-04  1.8674e+09\n",
      "     600  1.9415e+04  3.4066e+03  3.4066e+03  2.9802e-11  6.4261e-13  1.0000e+00  6.8868e-04  1.8674e+09\n",
      "     700  1.9415e+04  3.4066e+03  3.4066e+03  2.9802e-11  6.4261e-13  1.0000e+00  6.8868e-04  1.8674e+09\n",
      "     800  1.9415e+04  3.4066e+03  3.4066e+03  2.9802e-11  6.4261e-13  1.0000e+00  6.8868e-04  1.8674e+09\n",
      "     900  1.9415e+04  3.4066e+03  3.4066e+03  2.9802e-11  6.4261e-13  1.0000e+00  6.8868e-04  1.8674e+09\n",
      "    1000  1.9415e+04  3.4066e+03  3.4066e+03  2.9802e-11  6.4261e-13  1.0000e+00  6.8868e-04  1.8674e+09\n",
      "    1100  1.9415e+04  3.4066e+03  3.4066e+03  2.9802e-11  6.4261e-13  1.0000e+00  6.8868e-04  1.8674e+09\n",
      "    1200  1.9415e+04  3.4066e+03  3.4066e+03  2.9802e-11  6.4261e-13  1.0000e+00  6.8868e-04  1.8674e+09\n",
      "    1300  1.9415e+04  3.4066e+03  3.4066e+03  2.9802e-11  6.4261e-13  1.0000e+00  6.8868e-04  1.8674e+09\n",
      "    1400  1.9415e+04  3.4066e+03  3.4066e+03  2.9802e-11  6.4261e-13  1.0000e+00  6.8868e-04  1.8674e+09\n",
      "    1500  1.9415e+04  3.4066e+03  3.4066e+03  2.9802e-11  6.4261e-13  1.0000e+00  6.8868e-04  1.8674e+09\n",
      "    1600  1.9415e+04  3.4066e+03  3.4066e+03  2.9802e-11  6.4261e-13  1.0000e+00  6.8868e-04  1.8674e+09\n",
      "    1700  1.9415e+04  3.4066e+03  3.4066e+03  2.9802e-11  6.4261e-13  1.0000e+00  6.8868e-04  1.8674e+09\n",
      "    1800  1.9415e+04  3.4066e+03  3.4066e+03  2.9802e-11  6.4261e-13  1.0000e+00  6.8868e-04  1.8674e+09\n",
      "    1900  1.9415e+04  3.4066e+03  3.4066e+03  2.9802e-11  6.4261e-13  1.0000e+00  6.8868e-04  1.8674e+09\n",
      "    2000  1.9415e+04  3.4066e+03  3.4066e+03  2.9802e-11  6.4261e-13  1.0000e+00  6.8868e-04  1.8674e+09\n",
      "    2100  1.9415e+04  3.4066e+03  3.4066e+03  2.9802e-11  6.4261e-13  1.0000e+00  6.8868e-04  1.8674e+09\n",
      "    2200  1.9415e+04  3.4066e+03  3.4066e+03  2.9802e-11  6.4261e-13  1.0000e+00  6.8868e-04  1.8674e+09\n",
      "    2300  1.9415e+04  3.4066e+03  3.4066e+03  2.9802e-11  6.4261e-13  1.0000e+00  6.8868e-04  1.8674e+09\n",
      "    2400  1.9415e+04  3.4066e+03  3.4066e+03  2.9802e-11  6.4261e-13  1.0000e+00  6.8868e-04  1.8674e+09\n",
      "    2500  1.9415e+04  3.4066e+03  3.4066e+03  2.9802e-11  6.4261e-13  1.0000e+00  6.8868e-04  1.8674e+09\n",
      "    2600  1.9415e+04  3.4066e+03  3.4066e+03  2.9802e-11  6.4261e-13  1.0000e+00  6.8868e-04  1.8674e+09\n",
      "    2700  1.9415e+04  3.4066e+03  3.4066e+03  2.9802e-11  6.4261e-13  1.0000e+00  6.8868e-04  1.8674e+09\n",
      "    2800  1.9415e+04  3.4066e+03  3.4066e+03  2.9802e-11  6.4261e-13  1.0000e+00  6.8868e-04  1.8674e+09\n",
      "    2900  1.9415e+04  3.4066e+03  3.4066e+03  2.9802e-11  6.4261e-13  1.0000e+00  6.8868e-04  1.8674e+09\n",
      "    3000  1.9415e+04  3.4066e+03  3.4066e+03  2.9802e-11  6.4261e-13  1.0000e+00  6.8868e-04  1.8674e+09\n",
      "    3100  1.9415e+04  3.4066e+03  3.4066e+03  2.9802e-11  6.4261e-13  1.0000e+00  6.8868e-04  1.8674e+09\n",
      "    3200  1.9415e+04  3.4066e+03  3.4066e+03  2.9802e-11  6.4261e-13  1.0000e+00  6.8868e-04  1.8674e+09\n",
      "    3300  1.9415e+04  3.4066e+03  3.4066e+03  2.9802e-11  6.4261e-13  1.0000e+00  6.8868e-04  1.8674e+09\n",
      "    3400  1.9415e+04  3.4066e+03  3.4066e+03  2.9802e-11  6.4261e-13  1.0000e+00  6.8868e-04  1.8674e+09\n",
      "    3500  1.9415e+04  3.4066e+03  3.4066e+03  2.9802e-11  6.4261e-13  1.0000e+00  6.8868e-04  1.8674e+09\n",
      "    3600  1.9415e+04  3.4066e+03  3.4066e+03  2.9802e-11  6.4261e-13  1.0000e+00  6.8868e-04  1.8674e+09\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 41\u001B[0m\n\u001B[0;32m     38\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstate[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mdata\n\u001B[0;32m     40\u001B[0m \u001B[38;5;66;03m# Take a step inside optimizer\u001B[39;00m\n\u001B[1;32m---> 41\u001B[0m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;66;03m# Print out\u001B[39;00m\n\u001B[0;32m     44\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mprinterIteration(every\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\env1\\Lib\\site-packages\\torch\\optim\\optimizer.py:368\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    365\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[0;32m    366\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprofile_hook_step\u001B[39m(func: Callable[_P, R]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Callable[_P, R]:\n\u001B[1;32m--> 368\u001B[0m     \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    369\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs: _P\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: _P\u001B[38;5;241m.\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m R:\n\u001B[0;32m    370\u001B[0m         \u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m_ \u001B[38;5;241m=\u001B[39m args\n\u001B[0;32m    371\u001B[0m         \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m cast(Optimizer, \u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "## Main optimization loop\n",
    "\n",
    "iterations = 10000\n",
    "optimizer.printerHeader()\n",
    "for epoch in range(iterations):\n",
    "    \n",
    "    # Compute loss (objective)\n",
    "    net_bc_out = net(pt_x_bc, pt_t_bc) \n",
    "    loss = mse_cost_function(net_bc_out, pt_u_bc)\n",
    "    \n",
    "    # Compute gradient of objective\n",
    "    g = torch.zeros(n_parameters)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    i=0\n",
    "    for name, param in net.named_parameters():\n",
    "        grad_l = len(param.grad.view(-1))\n",
    "        g[i:i+grad_l] = param.grad.view(-1)\n",
    "        i += grad_l\n",
    "    \n",
    "    # Compute constraints\n",
    "    c = constraint_func(pt_x_collocation, pt_t_collocation, net).reshape(-1) \n",
    "\n",
    "    # Compute Jacobian\n",
    "    J = torch.zeros(n_constrs, n_parameters)\n",
    "    for i in range(n_constrs):\n",
    "        optimizer.zero_grad()\n",
    "        c[i].backward(retain_graph=True)\n",
    "        grads = torch.Tensor() #dict()\n",
    "        for name, param in net.named_parameters():\n",
    "            grads = torch.cat((grads, param.grad.view(-1)),0)\n",
    "        J[i,:] = grads\n",
    "\n",
    "    # Update f, g, c, J to optimizer\n",
    "    optimizer.state['J'] = J\n",
    "    optimizer.state['c'] = c.data\n",
    "    optimizer.state['g'] = g\n",
    "    optimizer.state['f'] = loss.data\n",
    "\n",
    "    # Take a step inside optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print out\n",
    "    optimizer.printerIteration(every=100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-12T13:54:05.639432400Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-12T13:54:05.642542400Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
