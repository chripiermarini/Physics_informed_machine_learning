output_folder: ./results        # Main output folder. All the outputs (log, plots, models) will be saved to this folder
sub_folders:                    # Subfolder names for model, plot, and log                  
  model : mdl
  plot  : plot
  log   : log
  
file_suffix: test               # str, all the leaf output file name will contain this suffix
n_epoch: 1000                  # Total number of epochs
save_loss_every: 10           # Save logs per [such] epochs
save_plot_model_every: 100           # Save model and plots per [such] epochs, n_epoch / save_plot_every better not exceed 500
stdout : 1                      # 0: print to screen, 1: print to log file
batch_seed : 5                       # number of runs; valid only when batch_size is not 'full'


problem:                        # Parameters for problem
  name  : Darcy
  regs:                           # regularization multipliers of different losses composing objective function
    pde     : 1.0e-2
    boundary: 1.0e-2
    fitting : 1
  n_constrs: 10                    # number of constraints ---
  constraint_type : pde       # pde, or boundary
  batch_size: 0.2            # str full or int batch_size #TODO
  nn_name       : FNOLocal           # neural network name. This should be a class in nn_architecture.py
  nn_parameters:                # The keys can be different for different neural network
    n_discretize : 16
    hidden_channels: 4
  dim_x: 2                      # int. dimension of domain
  x_max:  1                     # max x for all dim_x
  x_discretization: 16          # discretization of x for all dim_x
  n_train_group_pde_parameters: 1000     # int. number of group of pde parameters
  fitting_sample_group_percent: 0.2           # the fitting sample is interior points of all n_train_group_pde_parameters * [fitting_sample_group_percent]
  n_test_group_pde_parameters: 3     # int(50 at most). number of group of pde parameters 
  print_test_indices: [1]      # indicies of n_test_group_ped_parameters for plotting

optimizer:
  name : sqp                  # adam or sgd or sqp 
  lr    : 1.0e-2
  mu    : 1.0e-7
  beta2 : 0.999
  pretrain: null              # null; or dict with keys: epoch_start and file_suffix
  alpha_type: 'c_adam'         # 'ada', 'adam', 'c_ada', 'c_adam' 
  beta1 : 0.9
      
